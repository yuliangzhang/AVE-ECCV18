{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "em_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlo4V6MRBEDQ"
      },
      "source": [
        "# First things first\n",
        "Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMlNHfVxBEDT"
      },
      "source": [
        "# Expectation-maximization algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icS4MsxIBEDU"
      },
      "source": [
        "In this assignment, we will derive and implement formulas for Gaussian Mixture Model â€” one of the most commonly used methods for performing soft clustering of the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jequoJfSBEDV"
      },
      "source": [
        "### Setup\n",
        "Loading auxiliary files and importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZo-y9UBEDX"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    print(\"Downloading Colab files\")\n",
        "    ! shred -u setup_google_colab.py\n",
        "    ! wget https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/setup_google_colab.py -O setup_google_colab.py\n",
        "    import setup_google_colab\n",
        "    setup_google_colab.load_data_week2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urylZcbeBEDc"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import slogdet, det, solve\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.datasets import load_digits\n",
        "from w2_grader import EMGrader\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP8l9frZBEDf"
      },
      "source": [
        "### Grading\n",
        "We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvy3EOvBEDg"
      },
      "source": [
        "grader = EMGrader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL3A2sntBEDj"
      },
      "source": [
        "## Implementing EM for GMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPS_VdpBEDk"
      },
      "source": [
        "For debugging, we will use samples from a Gaussian mixture model with unknown mean, variance, and priors. We also added initial values of parameters for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9_aOn94BEDl"
      },
      "source": [
        "samples = np.load('samples.npz')\n",
        "X = samples['data']\n",
        "pi0 = samples['pi0']\n",
        "mu0 = samples['mu0']\n",
        "sigma0 = samples['sigma0']\n",
        "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmgeHTCfBEDp"
      },
      "source": [
        "### Reminder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgAwWi1nBEDq"
      },
      "source": [
        "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{p(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
        "\n",
        "<b>E-step</b>:<br>\n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
        "<b>M-step</b>:<br> \n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
        "\n",
        "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
        "\n",
        "Latent variables $T$ are indices of components to which each data point is assigned, i.e. $t_i$  is the cluster index for object $x_i$.\n",
        "\n",
        "The joint distribution can be written as follows: $\\log p(T, X \\mid \\theta) =  \\sum\\limits_{i=1}^N \\log p(t_i, x_i \\mid \\theta) = \\sum\\limits_{i=1}^N \\sum\\limits_{c=1}^C q(t_i = c) \\log \\left (\\pi_c \\, f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)\\right)$,\n",
        "where $f_{\\!\\mathcal{N}}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma_c|}}\n",
        "\\exp\\left(-\\frac{1}{2}({x}-{\\mu_c})^T{\\boldsymbol\\Sigma_c}^{-1}({x}-{\\mu_c})\n",
        "\\right)$ is the probability density function (pdf) of the normal distribution $\\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmi1nAtBEDr"
      },
      "source": [
        "### E-step\n",
        "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q_i(t_i) = p(t_i \\mid x_i, \\theta)$. We assume that $t_i$ equals to the cluster index of the true component of the $x_i$ object. To do so we need to compute $\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FR1RJ9BEDs"
      },
      "source": [
        "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{y_i}}{\\sum_j e^{y_j}}$, which is called _softmax_. When you compute exponents of large numbers, some numbers may become infinity. You can avoid this by dividing numerator and denominator by $e^{\\max(y)}$: $\\frac{e^{y_i-\\max(y)}}{\\sum_j e^{y_j - \\max(y)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. So, to compute desired formula you first subtract maximum value from each component in vector $\\mathbf{y}$ and then compute everything else as before.\n",
        "\n",
        "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to directly solve equation $Ay = x$ by using a special function. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by methods which do not explicitely invert the matrix. You can use ```np.linalg.solve``` for this.\n",
        "\n",
        "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL87Q8-TBEDu"
      },
      "source": [
        "<b>Task 1:</b> Implement E-step for GMM using template below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCUCaD28BEDw"
      },
      "source": [
        "def E_step(X, pi, mu, sigma):\n",
        "    \"\"\"\n",
        "    Performs E-step on GMM model\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    pi: (C), mixture component weights \n",
        "    mu: (C x d), mixture component means\n",
        "    sigma: (C x d x d), mixture component covariance matrices\n",
        "    \n",
        "    Returns:\n",
        "    gamma: (N x C), probabilities of clusters for objects\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = pi.shape[0] # number of clusters\n",
        "    d = mu.shape[1] # dimension of each object\n",
        "    gamma = np.zeros((N, C)) # distribution q(T)\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    return gamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlhktOlMBED1"
      },
      "source": [
        "gamma = E_step(X, pi0, mu0, sigma0)\n",
        "grader.submit_e_step(gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fERrQWSCBED5"
      },
      "source": [
        "### M-step\n",
        "\n",
        "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
        "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
        "\n",
        "<br>\n",
        "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A31OYSogBED6"
      },
      "source": [
        "<b>Task 2:</b> Implement M-step for GMM using template below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhOr5I1bBED7"
      },
      "source": [
        "def M_step(X, gamma):\n",
        "    \"\"\"\n",
        "    Performs M-step on GMM model\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    gamma: (N x C), distribution q(T)  \n",
        "    \n",
        "    Returns:\n",
        "    pi: (C)\n",
        "    mu: (C x d)\n",
        "    sigma: (C x d x d)\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = gamma.shape[1] # number of clusters\n",
        "    d = X.shape[1] # dimension of each object\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    return pi, mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i5cv65SBED-"
      },
      "source": [
        "gamma = E_step(X, pi0, mu0, sigma0)\n",
        "pi, mu, sigma = M_step(X, gamma)\n",
        "grader.submit_m_step(pi, mu, sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svyzjt7XBEEC"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qexOXBLUBEED"
      },
      "source": [
        "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
        "\n",
        "<b>Task 3:</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
        "\n",
        "$$\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{c=1}^{C} q(t_i =c) (\\log \\pi_c + \\log f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)) - \\sum_{i=1}^{N} \\sum_{c=1}^{K} q(t_i =c) \\log q(t_i =c)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5tKCZe0BEEE"
      },
      "source": [
        "def compute_vlb(X, pi, mu, sigma, gamma):\n",
        "    \"\"\"\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    gamma: (N x C), distribution q(T)  \n",
        "    pi: (C)\n",
        "    mu: (C x d)\n",
        "    sigma: (C x d x d)\n",
        "    \n",
        "    Returns value of variational lower bound\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = gamma.shape[1] # number of clusters\n",
        "    d = X.shape[1] # dimension of each object\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNva3XRTBEEI"
      },
      "source": [
        "pi, mu, sigma = pi0, mu0, sigma0\n",
        "gamma = E_step(X, pi, mu, sigma)\n",
        "pi, mu, sigma = M_step(X, gamma)\n",
        "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
        "grader.submit_VLB(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5p8XC-eBEEM"
      },
      "source": [
        "### Bringing it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQsdsNVCBEEU"
      },
      "source": [
        "Now that we have E step, M step and VLB, we can implement the training loop. We will initialize values of $\\pi$, $\\mu$ and $\\Sigma$ to some random numbers, train until $\\mathcal{L}$ stops changing, and return the resulting points. We also know that the EM algorithm converges to local optima. To find a better local optima, we will restart the algorithm multiple times from different (random) starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
        "\n",
        "Remember, that initial (random) values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
        "\n",
        "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to restart the procedure.\n",
        "\n",
        "<b>Task 4:</b> Implement training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1AAcyl0BEEW"
      },
      "source": [
        "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):\n",
        "    '''\n",
        "    Starts with random initialization *restarts* times\n",
        "    Runs optimization until saturation with *rtol* reached\n",
        "    or *max_iter* iterations were made.\n",
        "    \n",
        "    X: (N, d), data points\n",
        "    C: int, number of clusters\n",
        "    '''\n",
        "    N = X.shape[0] # number of objects\n",
        "    d = X.shape[1] # dimension of each object\n",
        "    best_loss = None\n",
        "    best_pi = None\n",
        "    best_mu = None\n",
        "    best_sigma = None\n",
        "\n",
        "    for _ in range(restarts):\n",
        "        try:\n",
        "            ### YOUR CODE HERE\n",
        "\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Singular matrix: components collapsed\")\n",
        "            pass\n",
        "\n",
        "    return best_loss, best_pi, best_mu, best_sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f8A5sbmBEEZ"
      },
      "source": [
        "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)\n",
        "grader.submit_EM(best_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYd6tPHKBEEd"
      },
      "source": [
        "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using a matrix $\\gamma$ computed on last E-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK_M6QLnBEEe"
      },
      "source": [
        "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
        "labels = gamma.argmax(axis=1)\n",
        "colors = np.array([(31, 119, 180), (255, 127, 14), (44, 160, 44)]) / 255.\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colors[labels], s=30)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4lnjrc7BEEl"
      },
      "source": [
        "# Authorization & Submission\n",
        "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate a token on this programming assignment's page. <b>Note:</b> The token expires 30 minutes after generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uQRl29_BEEl"
      },
      "source": [
        "STUDENT_EMAIL = # EMAIL HERE\n",
        "STUDENT_TOKEN = # TOKEN HERE\n",
        "grader.status()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCsqNOiBEEn"
      },
      "source": [
        "If you want to submit these answers, run cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sc9imWNBEEo"
      },
      "source": [
        "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}